{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vandercommonsdatabot.ipynb, a Python script for collecting Wikimedia Commons pageview data\n",
    "version = '0.1'\n",
    "created = '2021-12-14'\n",
    "\n",
    "# (c) 2021 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# IMPORTANT NOTE: If you hack this script to download your own data, you MUST change the user_agent_header\n",
    "# to your own URL and email address if you make modifications that affect how the script\n",
    "# interacts with the API. In particular, DO NOT decrease the value of api_sleep below 0.01 .\n",
    "\n",
    "# import, configuration, functions, etc.\n",
    "# Use pip install PyGithub to install the github module\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from time import sleep\n",
    "import csv\n",
    "import io\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import urllib.parse\n",
    "from github import Github\n",
    "\n",
    "# see Pageviews API information https://wikitech.wikimedia.org/wiki/Analytics/AQS/Pageviews\n",
    "# see Wikimedia REST API information https://wikimedia.org/api/rest_v1/\n",
    "\n",
    "# reference on PyGithub: https://pygithub.readthedocs.io/en/latest/github_objects/Repository.html\n",
    "# reference on GitHub API: https://developer.github.com/v3/guides/getting-started/\n",
    "\n",
    "api_sleep = 0.015 # number of seconds to wait between calls to the API, rate limit 100 calls/s\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "commons_page_prefix = 'https://commons.wikimedia.org/wiki/File:'\n",
    "\n",
    "github_username = ''  # set to empty string if using a token (for 2FA)\n",
    "organization_name = 'heardlibrary'\n",
    "organization_is_user = False\n",
    "repo_name = 'dashboard'\n",
    "cred_directory = 'home' # set to 'home' if the credential is in the home directory, otherwise working directory\n",
    "path_to_directory = 'gallery/'\n",
    "\n",
    "# -----------------\n",
    "# utility functions\n",
    "# -----------------\n",
    "\n",
    "# NOTE: change the user_agent_header string to something appropriate for your project\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderCommonsDataBot/' + version + ' (https://github.com/HeardLibrary/dashboard/tree/master/gallery; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "def yesterday_utc():\n",
    "    today = datetime.datetime.utcnow().toordinal() # get today as number of days from Jan 1, 1 CE\n",
    "    yesterday = datetime.datetime.fromordinal(today - 1) # turn ordinal day back into dateTime object\n",
    "    yesterday_iso = yesterday.strftime('%Y-%m-%d')\n",
    "    yesterday_wikimedia = yesterday.strftime('%Y%m%d')\n",
    "    return yesterday_iso, yesterday_wikimedia\n",
    "    \n",
    "def filename_to_commons_page_article(filename):\n",
    "    filename = filename.replace(' ', '_')\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = 'File:' + encoded_filename\n",
    "    url = url.replace('%28', '(').replace('%29', ')').replace('%2C', ',')\n",
    "    return url\n",
    "\n",
    "# read from a CSV file into a list of dictionaries (representing a table)\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        table = []\n",
    "        for row in dict_object:\n",
    "            table.append(row)\n",
    "    return table\n",
    "\n",
    "# read raw string from a file in GitHub\n",
    "def read_string_from_github_file(organization_name, repo_name, path_to_directory, filename):\n",
    "    path = path_to_directory + filename\n",
    "    r = requests.get('https://raw.githubusercontent.com/' + organization_name + '/' + repo_name + '/master/' + path)\n",
    "    return r.text\n",
    "\n",
    "# read from a CSV file in GitHub into a list of dictionaries (representing a table)\n",
    "def read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, filename):\n",
    "    path = path_to_directory + filename\n",
    "    r = requests.get('https://raw.githubusercontent.com/' + organization_name + '/' + repo_name + '/master/' + path)\n",
    "    file_text = r.text.split('\\n')\n",
    "    file_rows = csv.DictReader(file_text)\n",
    "    table = []\n",
    "    for row in file_rows:\n",
    "        table.append(row)\n",
    "    return table\n",
    "\n",
    "# read from a CSV file in GitHub into a list of lists (representing a table)\n",
    "def read_lists_from_github_csv(organization_name, repo_name, path_to_directory, filename):\n",
    "    path = path_to_directory + filename\n",
    "    r = requests.get('https://raw.githubusercontent.com/' + organization_name + '/' + repo_name + '/master/' + path)\n",
    "    file_text = r.text.split('\\n')\n",
    "    # remove any trailing newlines\n",
    "    if file_text[len(file_text)-1] == '':\n",
    "        file_text = file_text[0:len(file_text)-1]\n",
    "    file_rows = csv.reader(file_text)\n",
    "    table = []\n",
    "    for row in file_rows:\n",
    "        table.append(row)\n",
    "    return table\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_string(table, fieldnames):\n",
    "    output = io.StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in table:\n",
    "        writer.writerow(row)\n",
    "    return output.getvalue()\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def write_lists_to_string(table):\n",
    "    output = io.StringIO()\n",
    "    writer = csv.writer(output)\n",
    "    for row in table:\n",
    "        writer.writerow(row)\n",
    "    return output.getvalue()\n",
    "\n",
    "# -----------------\n",
    "# functions for interacting with APIs\n",
    "# -----------------\n",
    "\n",
    "# This function sends a request to the Wikimedia REST API to get pageviews for an article\n",
    "def get_pageview_counts(project, article, date):\n",
    "    # project is the subdomain (e.g. \"commons.wikimedia.org\")\n",
    "    # article is the page name after 'wiki/' in the URL (e.g. \"File:imagex.jpg\", or \"Q2\")\n",
    "    # date is in the format yyyymmdd\n",
    "    query_url = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/' + project + '/all-access/user/' + article + '/daily/' + date + '/' + date\n",
    "    #print(query_url)\n",
    "    r = requests.get(query_url, headers = generate_header_dictionary(accept_media_type))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        if 'items' in data:\n",
    "            #print('Found record successfully.')\n",
    "            value = str(data['items'][0]['views'])\n",
    "        else:\n",
    "            # Handle case where there were no views on that day or article didn't exist\n",
    "            #print('Did not find record.')\n",
    "            if data['title'] == 'Not found.':\n",
    "                value = str(0)\n",
    "            else:\n",
    "                value = r.text\n",
    "    except:\n",
    "        # Error messages not in JSON format\n",
    "        #print('Error with API response')\n",
    "        value = r.text\n",
    "    # delay to avoid hitting the API to rapidly\n",
    "    sleep(api_sleep)\n",
    "    return value\n",
    "\n",
    "# -----------------\n",
    "# functions for interacting with GitHub\n",
    "# -----------------\n",
    "\n",
    "# value of directory should be either 'home' or 'working'\n",
    "def load_credential(filename, directory):\n",
    "    cred = ''\n",
    "    # to change the script to look for the credential in the working directory, change the value of home to empty string\n",
    "    if directory == 'home':\n",
    "        home = str(Path.home()) #gets path to home directory; supposed to work for Win and Mac\n",
    "        credential_path = home + '/' + filename\n",
    "    else:\n",
    "        directory = 'working'\n",
    "        credential_path = filename\n",
    "    try:\n",
    "        with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "            cred = file_object.read()\n",
    "    except:\n",
    "        print(filename + ' file not found - is it in your ' + directory + ' directory?')\n",
    "        exit()\n",
    "    return(cred)\n",
    "\n",
    "# pass in an empty string for organization_name to use an individual account\n",
    "# pass in an empty string for github_username to use a token instead of username login\n",
    "def login_get_repo(repo_name, github_username, organization_name, organization_is_user, cred_directory):\n",
    "    if github_username == '':\n",
    "        token = load_credential('linked-data_github_token.txt', cred_directory)\n",
    "        g = Github(login_or_token = token)\n",
    "    else:\n",
    "        pwd = load_credential('pwd.txt', cred_directory)\n",
    "        g = Github(github_username, pwd)\n",
    "    \n",
    "    if organization_is_user:\n",
    "        # this option accesses a user's repo instead of an organizational one\n",
    "        # In this case, the value of organization_name is not used.\n",
    "        user = g.get_user()\n",
    "        repo = user.get_repo(repo_name)\n",
    "    else:\n",
    "        # this option creates an instance of a repo in an organization\n",
    "        # to which the token creator has push access\n",
    "        organization = g.get_organization(organization_name)\n",
    "        repo = organization.get_repo(repo_name)\n",
    "    return(repo)\n",
    "\n",
    "def get_user_list(repo):\n",
    "    person_list = []\n",
    "    people = repo.get_collaborators()\n",
    "    for person in people:\n",
    "        person_list.append(person.login)\n",
    "    return person_list\n",
    "\n",
    "def get_file_sha(account, repo, file_path):\n",
    "    # get the data about the file to get its blob SHA\n",
    "\n",
    "    r = requests.get('https://api.github.com/repos/' + account + '/' + repo + '/contents/' + file_path)\n",
    "    file_data = r.json()\n",
    "    try:\n",
    "        sha = file_data['sha']\n",
    "    except:\n",
    "        # if the file doesn't already exist on GitHub, no sha will be returned\n",
    "        sha = ''\n",
    "    return sha\n",
    "\n",
    "# use this function to update an existing text file\n",
    "def update_file(account, repo_name, path_to_directory, filename, content):\n",
    "    path = path_to_directory + filename\n",
    "    commit_message = 'Update ' + filename + ' file via API'\n",
    "    sha = get_file_sha(account, repo_name, path)\n",
    "    if sha == '':\n",
    "        response = repo.create_file(path, commit_message, content)\n",
    "    else:\n",
    "        response = repo.update_file(path, commit_message, content, sha)\n",
    "    return response\n",
    "\n",
    "# -----------------\n",
    "# top-level functions for acquiring the main dataset\n",
    "# -----------------\n",
    "\n",
    "# Retrieves the total contributions for all of the participants in the VandyCite project\n",
    "# If it fails due to timeout or some other error, the table remains unchanged\n",
    "# Returns a raw CSV string\n",
    "def get_commons_pageview_counts(organization_name, repo_name, path_to_directory, table):\n",
    "\n",
    "    # Get Commons image data\n",
    "    user_dicts = read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, 'commons_images.csv')\n",
    "\n",
    "    # Create column headers list\n",
    "    mid_list = [] # M IDs are the Commons equivalents of Q IDs used by the Structured data Wikibase\n",
    "    for dict_record in user_dicts:\n",
    "        mid_list.append(dict_record['commons_id'])\n",
    "        \n",
    "    # Retrieve data from the Wikimedia REST API\n",
    "    project = 'commons.wikimedia.org'\n",
    "    yesterday_iso, yesterday_wikimedia = yesterday_utc()\n",
    "    #yesterday_iso = '2021-12-11' # uncomment to override date to be checked\n",
    "    #yesterday_wikimedia = '20211211' # uncomment to override date to be checked\n",
    "\n",
    "    fieldnames = ['date', 'total'] + mid_list\n",
    "    row_dict = {'date': yesterday_iso}\n",
    "\n",
    "    total = 0\n",
    "    for dict_record in user_dicts:\n",
    "        image_filename = dict_record['image_name']\n",
    "        print(image_filename)\n",
    "        tries = 0\n",
    "        success = False\n",
    "        # try to acquire the data for an hour\n",
    "        while (success == False) and (tries < 12):\n",
    "            try:\n",
    "                count = get_pageview_counts(project, filename_to_commons_page_article(image_filename), yesterday_wikimedia)\n",
    "                success = True\n",
    "                row_dict[dict_record['commons_id']] = count\n",
    "                total += int(count)\n",
    "            except:\n",
    "                tries += 1\n",
    "                sleep(300) # wait 5 minutes and try again\n",
    "    row_dict['total'] = str(total)\n",
    "\n",
    "    if success:\n",
    "        table.append(row_dict)\n",
    "\n",
    "    #print(json.dumps(table, indent = 2))\n",
    "    return write_dicts_to_string(table, fieldnames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True: # infinite loop\n",
    "    try:\n",
    "        print('Time checked:', datetime.datetime.utcnow().isoformat())\n",
    "\n",
    "        date_last_run = read_string_from_github_file(organization_name, repo_name, path_to_directory, 'last_run.txt')\n",
    "        print('Date last run:', date_last_run)\n",
    "\n",
    "        date_now_utc = generate_utc_date()\n",
    "        print('UTC date now is:', date_now_utc)\n",
    "\n",
    "        if date_now_utc > date_last_run:\n",
    "            # log into the GitHub API and create a repo instance\n",
    "            repo = login_get_repo(repo_name, github_username, organization_name, organization_is_user, cred_directory)\n",
    "\n",
    "            # Record today's counts of contrubutions to Wikidata by VandyCite members\n",
    "            filename = 'commons_pageview_data.csv'\n",
    "            table = read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, filename)\n",
    "            rawCsvText = get_commons_pageview_counts(organization_name, repo_name, path_to_directory, table)\n",
    "            response = update_file(organization_name, repo_name, path_to_directory, filename, rawCsvText)\n",
    "            print(response)\n",
    "\n",
    "            # Update the date last run\n",
    "            response = update_file(organization_name, repo_name, path_to_directory, 'last_run.txt', generate_utc_date() )\n",
    "            print('done')\n",
    "        print()\n",
    "        # wait an hour before checking again\n",
    "        sleep(3600)\n",
    "    except:\n",
    "        print('Error occurred, trying again in 10 minutes')\n",
    "        sleep(600)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
