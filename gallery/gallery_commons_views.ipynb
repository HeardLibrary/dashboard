{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, configuration, functions, etc.\n",
    "# Use pip install PyGithub to install the github module\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from time import sleep\n",
    "import csv\n",
    "import io\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from github import Github\n",
    "\n",
    "# see Pageviews API information https://wikitech.wikimedia.org/wiki/Analytics/AQS/Pageviews\n",
    "# see Wikimedia REST API information https://wikimedia.org/api/rest_v1/\n",
    "\n",
    "# reference on PyGithub: https://pygithub.readthedocs.io/en/latest/github_objects/Repository.html\n",
    "# reference on GitHub API: https://developer.github.com/v3/guides/getting-started/\n",
    "\n",
    "api_sleep = 0.015 # number of seconds to wait between calls to the API, limit 100 calls/s\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "github_username = ''  # set to empty string if using a token (for 2FA)\n",
    "organization_name = 'heardlibrary'\n",
    "organization_is_user = False\n",
    "repo_name = 'dashboard'\n",
    "cred_directory = 'home' # set to 'home' if the credential is in the home directory, otherwise working directory\n",
    "path_to_directory = 'gallery/'\n",
    "\n",
    "# -----------------\n",
    "# utility functions\n",
    "# -----------------\n",
    "\n",
    "# NOTE: change the user_agent_header string to something appropriate for your project\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderDataBot/0.1 (https://github.com/HeardLibrary/linked-data/tree/master/publications/data; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# read from a CSV file into a list of dictionaries (representing a table)\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        table = []\n",
    "        for row in dict_object:\n",
    "            table.append(row)\n",
    "    return table\n",
    "\n",
    "# read raw string from a file in GitHub\n",
    "def read_string_from_github_file(organization_name, repo_name, path_to_directory, filename):\n",
    "    path = path_to_directory + filename\n",
    "    r = requests.get('https://raw.githubusercontent.com/' + organization_name + '/' + repo_name + '/master/' + path)\n",
    "    return r.text\n",
    "\n",
    "# read from a CSV file in GitHub into a list of dictionaries (representing a table)\n",
    "def read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, filename):\n",
    "    path = path_to_directory + filename\n",
    "    r = requests.get('https://raw.githubusercontent.com/' + organization_name + '/' + repo_name + '/master/' + path)\n",
    "    file_text = r.text.split('\\n')\n",
    "    file_rows = csv.DictReader(file_text)\n",
    "    table = []\n",
    "    for row in file_rows:\n",
    "        table.append(row)\n",
    "    return table\n",
    "\n",
    "# read from a CSV file in GitHub into a list of lists (representing a table)\n",
    "def read_lists_from_github_csv(organization_name, repo_name, path_to_directory, filename):\n",
    "    path = path_to_directory + filename\n",
    "    r = requests.get('https://raw.githubusercontent.com/' + organization_name + '/' + repo_name + '/master/' + path)\n",
    "    file_text = r.text.split('\\n')\n",
    "    # remove any trailing newlines\n",
    "    if file_text[len(file_text)-1] == '':\n",
    "        file_text = file_text[0:len(file_text)-1]\n",
    "    file_rows = csv.reader(file_text)\n",
    "    table = []\n",
    "    for row in file_rows:\n",
    "        table.append(row)\n",
    "    return table\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_string(table, fieldnames):\n",
    "    output = io.StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in table:\n",
    "        writer.writerow(row)\n",
    "    return output.getvalue()\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def write_lists_to_string(table):\n",
    "    output = io.StringIO()\n",
    "    writer = csv.writer(output)\n",
    "    for row in table:\n",
    "        writer.writerow(row)\n",
    "    return output.getvalue()\n",
    "\n",
    "# -----------------\n",
    "# functions for interacting with APIs\n",
    "# -----------------\n",
    "\n",
    "# This function sends a query to a SPARQL endpoint and returns a single value.\n",
    "# For the Wikidata SPARQL endpoint, it extracts \"single_value\" from the query.\n",
    "def get_single_value(query, endpoint_url):\n",
    "    r = requests.get(endpoint_url, params={'query' : query}, headers = generate_header_dictionary(accept_media_type))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        #print(json.dumps(data, indent=2))\n",
    "        \n",
    "        # Extract value from response JSON depending on the API type\n",
    "        if endpoint_url == 'https://query.wikidata.org/sparql':\n",
    "            value = data['results']['bindings'][0]['single_value']['value']\n",
    "    except:\n",
    "        value = [r.text]\n",
    "    # delay to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(api_sleep)\n",
    "    return value\n",
    "\n",
    "# This function sends a query to the Wikidata SPARQL endpoint that searches for\n",
    "# counts related to all subsidiary units of Vanderbilt. The function returns a list of\n",
    "# dictionaries with the Q ID and count for each unit.\n",
    "def get_unit_counts(query):\n",
    "    table = []\n",
    "    endpoint_url = 'https://query.wikidata.org/sparql'\n",
    "    accept_media_type = 'application/json'\n",
    "    r = requests.get(endpoint_url, params={'query' : query}, headers = generate_header_dictionary(accept_media_type))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            unit_iri = statement['unit']['value']\n",
    "            unit_qnumber = extract_qnumber(unit_iri)\n",
    "            count = statement['count']['value']\n",
    "            table.append({'unit': unit_qnumber, 'count': count})\n",
    "    except:\n",
    "        table = [r.text]\n",
    "    # delay to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(api_sleep)\n",
    "    return table\n",
    "\n",
    "# This function sends a query to the XTools Edit Counter and returns a single value.\n",
    "def get_xtools_edit_counts(username, project, namespace):\n",
    "    query_url = 'https://xtools.wmflabs.org/api/user/simple_editcount/' + project + '/' + username + '/' + namespace\n",
    "    #query_url = 'https://xtools.wmflabs.org/api/user/simple_editcount/' + project + '/' + username + '/' + namespace + '/' + start_date + '/' + end_date\n",
    "    r = requests.get(query_url, headers = generate_header_dictionary(accept_media_type))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        #print(json.dumps(data, indent=2))\n",
    "        \n",
    "        value = data['live_edit_count']\n",
    "    except:\n",
    "        value = [r.text]\n",
    "    # delay to avoid hitting the API to rapidly\n",
    "    sleep(api_sleep)\n",
    "    return value\n",
    "\n",
    "# This function sends a query to the XTools User Pages counter and returns a single value.\n",
    "def get_xtools_page_creation_counts(username, project_url):\n",
    "    query_url = 'https://xtools.wmflabs.org/api/user/pages_count/' + project_url + '/' + username\n",
    "    r = requests.get(query_url, headers = generate_header_dictionary(accept_media_type))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        #print(json.dumps(data, indent=2))\n",
    "        if 'count' in data['counts']:\n",
    "            value = data['counts']['count']\n",
    "        else:\n",
    "            value = 0\n",
    "    except:\n",
    "        value = [r.text]\n",
    "    # delay to avoid hitting the API to rapidly\n",
    "    sleep(api_sleep)\n",
    "    return value\n",
    "\n",
    "# -----------------\n",
    "# functions for interacting with GitHub\n",
    "# -----------------\n",
    "\n",
    "# value of directory should be either 'home' or 'working'\n",
    "def load_credential(filename, directory):\n",
    "    cred = ''\n",
    "    # to change the script to look for the credential in the working directory, change the value of home to empty string\n",
    "    if directory == 'home':\n",
    "        home = str(Path.home()) #gets path to home directory; supposed to work for Win and Mac\n",
    "        credential_path = home + '/' + filename\n",
    "    else:\n",
    "        directory = 'working'\n",
    "        credential_path = filename\n",
    "    try:\n",
    "        with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "            cred = file_object.read()\n",
    "    except:\n",
    "        print(filename + ' file not found - is it in your ' + directory + ' directory?')\n",
    "        exit()\n",
    "    return(cred)\n",
    "\n",
    "# pass in an empty string for organization_name to use an individual account\n",
    "# pass in an empty string for github_username to use a token instead of username login\n",
    "def login_get_repo(repo_name, github_username, organization_name, organization_is_user, cred_directory):\n",
    "    if github_username == '':\n",
    "        token = load_credential('linked-data_github_token.txt', cred_directory)\n",
    "        g = Github(login_or_token = token)\n",
    "    else:\n",
    "        pwd = load_credential('pwd.txt', cred_directory)\n",
    "        g = Github(github_username, pwd)\n",
    "    \n",
    "    if organization_is_user:\n",
    "        # this option accesses a user's repo instead of an organizational one\n",
    "        # In this case, the value of organization_name is not used.\n",
    "        user = g.get_user()\n",
    "        repo = user.get_repo(repo_name)\n",
    "    else:\n",
    "        # this option creates an instance of a repo in an organization\n",
    "        # to which the token creator has push access\n",
    "        organization = g.get_organization(organization_name)\n",
    "        repo = organization.get_repo(repo_name)\n",
    "    return(repo)\n",
    "\n",
    "def get_user_list(repo):\n",
    "    person_list = []\n",
    "    people = repo.get_collaborators()\n",
    "    for person in people:\n",
    "        person_list.append(person.login)\n",
    "    return person_list\n",
    "\n",
    "def get_file_sha(account, repo, file_path):\n",
    "    # get the data about the file to get its blob SHA\n",
    "\n",
    "    r = requests.get('https://api.github.com/repos/' + account + '/' + repo + '/contents/' + file_path)\n",
    "    file_data = r.json()\n",
    "    try:\n",
    "        sha = file_data['sha']\n",
    "    except:\n",
    "        # if the file doesn't already exist on GitHub, no sha will be returned\n",
    "        sha = ''\n",
    "    return sha\n",
    "\n",
    "# use this function to update an existing text file\n",
    "def update_file(account, repo_name, path_to_directory, filename, content):\n",
    "    path = path_to_directory + filename\n",
    "    commit_message = 'Update ' + filename + ' file via API'\n",
    "    sha = get_file_sha(account, repo_name, path)\n",
    "    if sha == '':\n",
    "        response = repo.create_file(path, commit_message, content)\n",
    "    else:\n",
    "        response = repo.update_file(path, commit_message, content, sha)\n",
    "    return response\n",
    "\n",
    "# -----------------\n",
    "# top-level functions for acquiring the main datasets\n",
    "# -----------------\n",
    "\n",
    "# Retrieves the total contributions for all of the participants in the VandyCite project\n",
    "# If it fails due to timeout or some other error, the table remains unchanged\n",
    "# Returns a raw CSV string\n",
    "def get_vandycite_contribution_counts(organization_name, repo_name, path_to_directory, table):\n",
    "    # Get username list\n",
    "    vandycite_user_list = []\n",
    "    user_dicts = read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, 'vandycite_users.csv')\n",
    "    for dict in user_dicts:\n",
    "        vandycite_user_list.append(dict['username'])\n",
    "\n",
    "    # Retrieve data from XTools Edit Counter API\n",
    "    project = 'wikidata'\n",
    "    namespace = '0' # 0 is the main namespace\n",
    "\n",
    "    fieldnames = ['date'] + vandycite_user_list + ['total']\n",
    "    today = generate_utc_date()\n",
    "    row_dict = {'date': today}\n",
    "\n",
    "    total = 0\n",
    "    for username in vandycite_user_list:\n",
    "        print(username)\n",
    "        tries = 0\n",
    "        success = False\n",
    "        # try to acquire the data for an hour\n",
    "        while (success == False) and (tries < 12):\n",
    "            try:\n",
    "                count = get_xtools_edit_counts(username, project, namespace)\n",
    "                success = True\n",
    "                row_dict[username] = count\n",
    "                total += int(count)\n",
    "            except:\n",
    "                tries += 1\n",
    "                sleep(300) # wait 5 minutes and try again\n",
    "    row_dict['total'] = str(total)\n",
    "    \n",
    "    if success:\n",
    "        table.append(row_dict)\n",
    "\n",
    "    return write_dicts_to_string(table, fieldnames)\n",
    "\n",
    "\n",
    "# Returns a raw CSV string\n",
    "def get_vandycite_page_creation_counts(organization_name, repo_name, path_to_directory, table):\n",
    "    # Get username list\n",
    "    vandycite_user_list = []\n",
    "    user_dicts = read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, 'vandycite_users.csv')\n",
    "    for dict in user_dicts:\n",
    "        vandycite_user_list.append(dict['username'])\n",
    "\n",
    "    # Retrieve data from XTools User Pages API\n",
    "    project_url = 'www.wikidata.org'\n",
    "\n",
    "    fieldnames = ['date'] + vandycite_user_list + ['total']\n",
    "    today = generate_utc_date()\n",
    "    row_dict = {'date': today}\n",
    "\n",
    "    total = 0\n",
    "    for username in vandycite_user_list:\n",
    "        print(username)\n",
    "        tries = 0\n",
    "        success = False\n",
    "        # try to acquire the data for an hour\n",
    "        while (success == False) and (tries < 12):\n",
    "            try:\n",
    "                count = get_xtools_page_creation_counts(username, project_url)\n",
    "                success = True\n",
    "                row_dict[username] = count\n",
    "                total += int(count)\n",
    "            except:\n",
    "                tries += 1\n",
    "                sleep(300) # wait 5 minutes and try again\n",
    "    row_dict['total'] = str(total)\n",
    "    \n",
    "    if success:\n",
    "        table.append(row_dict)\n",
    "\n",
    "    return write_dicts_to_string(table, fieldnames)\n",
    "\n",
    "\n",
    "\n",
    "# Runs all of the queries that retrieve a single value for the whole university\n",
    "# If it fails due to timeout or some other error, the table remains unchanged\n",
    "# Returns a raw CSV string\n",
    "def get_vu_counts(table):\n",
    "    all_vu_query_list = [\n",
    "        {'name': 'vu_total',\n",
    "        'query': '''\n",
    "        select (count(distinct ?person) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_men',\n",
    "        'query': '''\n",
    "        select (count(distinct ?man) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?man wdt:P1416 ?unit.\n",
    "          ?man wdt:P21 wd:Q6581097.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_women',\n",
    "        'query': '''\n",
    "        select (count(distinct ?woman) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?woman wdt:P1416 ?unit.\n",
    "          ?woman wdt:P21 wd:Q6581072.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_orcid',\n",
    "        'query': '''\n",
    "        select (count(distinct ?person) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          ?person wdt:P496 ?orcid.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_works',\n",
    "        'query': '''\n",
    "        select (count(distinct ?work) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          ?work wdt:P50 ?person.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_men_works',\n",
    "        'query': '''\n",
    "        select (count(distinct ?work) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?man wdt:P1416 ?unit.\n",
    "          ?man wdt:P21 wd:Q6581097.\n",
    "          ?work wdt:P50 ?man.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_women_works',\n",
    "        'query': '''\n",
    "        select (count(distinct ?work) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?woman wdt:P1416 ?unit.\n",
    "          ?woman wdt:P21 wd:Q6581072.\n",
    "          ?work wdt:P50 ?woman.\n",
    "          }\n",
    "        '''},\n",
    "    ]\n",
    "    #print(json.dumps(all_vu_query_list, indent=2))\n",
    "\n",
    "    # Retrieve data from Wikidata Query Service\n",
    "    endpoint_url = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "    fieldnames = ['date']\n",
    "    today = generate_utc_date()\n",
    "    row_dict = {'date': today}\n",
    "\n",
    "    for query_dict in all_vu_query_list:\n",
    "        query_name = query_dict['name']\n",
    "        print(query_name)\n",
    "        fieldnames.append(query_name)\n",
    "        tries = 0\n",
    "        success = False\n",
    "\n",
    "        # try to acquire the data for an hour\n",
    "        while (success == False) and (tries < 12):\n",
    "            try:\n",
    "                count = get_single_value(query_dict['query'], endpoint_url)\n",
    "                success = True\n",
    "                row_dict[query_name] = count\n",
    "            except:\n",
    "                tries += 1\n",
    "                sleep(300) # wait 5 minutes and try again\n",
    "    if success:\n",
    "        table.append(row_dict)\n",
    "\n",
    "    return write_dicts_to_string(table, fieldnames)\n",
    "\n",
    "def get_unit_affiliation_queries():\n",
    "    units_query_list = [\n",
    "        {'name': 'units_total',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?person) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''},\n",
    "        {'name': 'units_women',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?woman) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?woman wdt:P1416 ?unit.\n",
    "          ?woman wdt:P21 wd:Q6581072.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''},\n",
    "        {'name': 'units_men',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?man) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?man wdt:P1416 ?unit.\n",
    "          ?man wdt:P21 wd:Q6581097.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''},\n",
    "        {'name': 'units_orcid',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?person) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          ?person wdt:P496 ?orcid.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''},\n",
    "        {'name': 'units_works',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?work) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          ?work wdt:P50 ?person.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''},\n",
    "        {'name': 'units_works_men',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?work) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?man wdt:P1416 ?unit.\n",
    "          ?man wdt:P21 wd:Q6581097.\n",
    "          ?work wdt:P50 ?man.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''},\n",
    "        {'name': 'units_works_women',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?work) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?woman wdt:P1416 ?unit.\n",
    "          ?woman wdt:P21 wd:Q6581072.\n",
    "          ?work wdt:P50 ?woman.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''}\n",
    "    ]\n",
    "    return units_query_list\n",
    "\n",
    "# This retrieves counts by unit for a particular query type, then appends the results\n",
    "# for all of the units as a new row in the table.\n",
    "# NOTE: unlike the other functions, the table here is a list of lists, not list of dicts.\n",
    "# The first column must be the date.\n",
    "def add_query_to_unit_table(table, query):\n",
    "    date = generate_utc_date()\n",
    "    tries = 0\n",
    "    success = False\n",
    "\n",
    "    # try to acquire the data for an hour\n",
    "    while (success == False) and (tries < 12):\n",
    "        try:\n",
    "            dictionary = get_unit_counts(query)\n",
    "            success = True\n",
    "            row_list = [date]\n",
    "            for header in table[0][1:len(table[0])]: # skip the first item (date)\n",
    "                found = False\n",
    "                for count in dictionary:\n",
    "                    if count['unit'] == header:\n",
    "                        found = True\n",
    "                        row_list.append(count['count'])\n",
    "                if not found:\n",
    "                    row_list.append('0')\n",
    "        except:\n",
    "            tries += 1\n",
    "            sleep(300) # wait 5 minutes and try again\n",
    "    if success:\n",
    "        table.append(row_list)\n",
    "    return write_lists_to_string(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time checked: 2021-12-10T11:55:21.982702\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-10\n",
      "\n",
      "Time checked: 2021-12-10T12:55:22.086538\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-10\n",
      "\n",
      "Time checked: 2021-12-10T13:55:22.306986\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-10\n",
      "\n",
      "Time checked: 2021-12-10T14:55:22.442979\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-10\n",
      "\n",
      "Time checked: 2021-12-10T15:55:22.596162\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-10\n",
      "\n",
      "Time checked: 2021-12-10T16:55:22.901563\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-10\n",
      "\n",
      "Time checked: 2021-12-10T17:55:23.034135\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-10\n",
      "\n",
      "Time checked: 2021-12-10T18:55:23.277326\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-10\n",
      "\n",
      "Time checked: 2021-12-10T19:55:23.781459\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-10\n",
      "\n",
      "Time checked: 2021-12-10T20:55:24.034177\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-10\n",
      "\n",
      "Time checked: 2021-12-10T21:55:24.124778\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-10\n",
      "\n",
      "Time checked: 2021-12-10T22:55:24.357778\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-10\n",
      "\n",
      "Time checked: 2021-12-10T23:55:24.422940\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-10\n",
      "\n",
      "Time checked: 2021-12-11T00:55:24.585124\n",
      "Date last run: 2021-12-10\n",
      "UTC date now is: 2021-12-11\n",
      "Item counts (university-wide):\n",
      "vu_total\n",
      "vu_men\n",
      "vu_women\n",
      "vu_orcid\n",
      "vu_works\n",
      "vu_men_works\n",
      "vu_women_works\n",
      "{'commit': Commit(sha=\"9fa1553bd624b211864797a09c33803be6de3740\"), 'content': ContentFile(path=\"vandycite/vandycite_item_data.csv\")}\n",
      "\n",
      "Item counts by unit:\n",
      "units_total\n",
      "{'commit': Commit(sha=\"9eadcd847c0a7ba808cbac08439f45bd0beb3f8b\"), 'content': ContentFile(path=\"vandycite/units_total.csv\")}\n",
      "units_women\n",
      "{'commit': Commit(sha=\"6c324650122046e6377dff0dc61b8dbef838da20\"), 'content': ContentFile(path=\"vandycite/units_women.csv\")}\n",
      "units_men\n",
      "{'commit': Commit(sha=\"8d486ea1c8c4bb32a08dfcb335d6093305914ae0\"), 'content': ContentFile(path=\"vandycite/units_men.csv\")}\n",
      "units_orcid\n",
      "{'commit': Commit(sha=\"2a40fec2c1a350ba97ee4c999e8741229ab06673\"), 'content': ContentFile(path=\"vandycite/units_orcid.csv\")}\n",
      "units_works\n",
      "{'commit': Commit(sha=\"13c8ea7aa3ee0cb3e02da8eb18b3f76f6219de26\"), 'content': ContentFile(path=\"vandycite/units_works.csv\")}\n",
      "units_works_men\n",
      "{'commit': Commit(sha=\"2f09eb92d557447e1f4119d026f8d29fa74d3dea\"), 'content': ContentFile(path=\"vandycite/units_works_men.csv\")}\n",
      "units_works_women\n",
      "{'commit': Commit(sha=\"736d8b1b6dbc61db4a6e9f24ec2bd065a7928f99\"), 'content': ContentFile(path=\"vandycite/units_works_women.csv\")}\n",
      "\n",
      "Contributions\n",
      "Clifford_Anderson\n",
      "Baskaufs\n",
      "Fmlester\n",
      "Ramonavromero\n",
      "Talinum\n",
      "Celiaswalker\n",
      "CatonMA2\n",
      "Gridersd\n",
      "JeffBTaylor\n",
      "Marjans74\n",
      "Charlotte_Y._Lew\n",
      "KukanaLuika\n",
      "Loyodea\n",
      "VanderBot\n",
      "{'commit': Commit(sha=\"7d5ac36949ad2f8b1063535d7426df221a629f26\"), 'content': ContentFile(path=\"vandycite/vandycite_edit_data.csv\")}\n",
      "Pages created\n",
      "Clifford_Anderson\n",
      "Baskaufs\n",
      "Fmlester\n",
      "Ramonavromero\n",
      "Talinum\n",
      "Celiaswalker\n",
      "CatonMA2\n",
      "Gridersd\n",
      "JeffBTaylor\n",
      "Marjans74\n",
      "Charlotte_Y._Lew\n",
      "KukanaLuika\n",
      "Loyodea\n",
      "VanderBot\n",
      "{'commit': Commit(sha=\"475251380dd9ecb90db73ecacecd611f56628957\"), 'content': ContentFile(path=\"vandycite/vandycite_page_creation_data.csv\")}\n",
      "done\n",
      "\n",
      "Time checked: 2021-12-11T01:56:17.737524\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T02:56:18.204086\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T03:56:18.344301\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T04:56:18.493161\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T05:56:18.612693\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T06:56:18.757260\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T07:56:18.877659\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T12:38:16.450961\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T14:11:30.058832\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T15:11:30.216327\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T16:11:30.363845\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T17:11:30.533985\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T18:11:30.591477\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T19:11:30.710641\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T20:11:30.864109\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T21:11:31.025636\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T22:11:31.378521\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-11T23:11:31.469983\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-11\n",
      "\n",
      "Time checked: 2021-12-12T00:11:31.635038\n",
      "Date last run: 2021-12-11\n",
      "UTC date now is: 2021-12-12\n",
      "Item counts (university-wide):\n",
      "vu_total\n",
      "vu_men\n",
      "vu_women\n",
      "vu_orcid\n",
      "vu_works\n",
      "vu_men_works\n",
      "vu_women_works\n",
      "{'commit': Commit(sha=\"042d390288ec857f9c75153f259a845be100523e\"), 'content': ContentFile(path=\"vandycite/vandycite_item_data.csv\")}\n",
      "\n",
      "Item counts by unit:\n",
      "units_total\n",
      "{'commit': Commit(sha=\"95de28752d44f517eb3fe801ef33c3ad775da3f4\"), 'content': ContentFile(path=\"vandycite/units_total.csv\")}\n",
      "units_women\n",
      "{'commit': Commit(sha=\"4e5c8e4913a9fd98f3f8e55c8e4ca7073a29130f\"), 'content': ContentFile(path=\"vandycite/units_women.csv\")}\n",
      "units_men\n",
      "{'commit': Commit(sha=\"ecda6bc3eed606e02efdcba17e767cdff2457da0\"), 'content': ContentFile(path=\"vandycite/units_men.csv\")}\n",
      "units_orcid\n",
      "{'commit': Commit(sha=\"1fb0c32c7c35533d35007205c4b069c7309d5761\"), 'content': ContentFile(path=\"vandycite/units_orcid.csv\")}\n",
      "units_works\n",
      "{'commit': Commit(sha=\"5c7b3f623623960cf9c1b6a2d48204071dc2f7bb\"), 'content': ContentFile(path=\"vandycite/units_works.csv\")}\n",
      "units_works_men\n",
      "{'commit': Commit(sha=\"4f18b0ddff40ae43ea15da0ec31ee00d8d3219a8\"), 'content': ContentFile(path=\"vandycite/units_works_men.csv\")}\n",
      "units_works_women\n",
      "{'commit': Commit(sha=\"ce30d8291e25eff64eafe62e42c4fa930fce7127\"), 'content': ContentFile(path=\"vandycite/units_works_women.csv\")}\n",
      "\n",
      "Contributions\n",
      "Clifford_Anderson\n",
      "Baskaufs\n",
      "Fmlester\n",
      "Ramonavromero\n",
      "Talinum\n",
      "Celiaswalker\n",
      "CatonMA2\n",
      "Gridersd\n",
      "JeffBTaylor\n",
      "Marjans74\n",
      "Charlotte_Y._Lew\n",
      "KukanaLuika\n",
      "Loyodea\n",
      "VanderBot\n",
      "{'commit': Commit(sha=\"ca6b5b873e1824b2834361844f5f4a1d9dbd87a2\"), 'content': ContentFile(path=\"vandycite/vandycite_edit_data.csv\")}\n",
      "Pages created\n",
      "Clifford_Anderson\n",
      "Baskaufs\n",
      "Fmlester\n",
      "Ramonavromero\n",
      "Talinum\n",
      "Celiaswalker\n",
      "CatonMA2\n",
      "Gridersd\n",
      "JeffBTaylor\n",
      "Marjans74\n",
      "Charlotte_Y._Lew\n",
      "KukanaLuika\n",
      "Loyodea\n",
      "VanderBot\n",
      "{'commit': Commit(sha=\"5eced1b2be23cf580944551fe15ea063f8c97e81\"), 'content': ContentFile(path=\"vandycite/vandycite_page_creation_data.csv\")}\n",
      "done\n",
      "\n",
      "Time checked: 2021-12-12T01:12:15.217882\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T02:12:15.252332\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T03:12:15.690998\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T04:12:15.909097\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T05:12:16.049352\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T06:12:16.460038\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T07:12:16.508647\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T08:12:16.676511\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T09:12:16.942658\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T10:12:17.067756\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T11:12:17.197183\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T12:12:17.589573\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T13:12:17.812471\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T14:12:18.071836\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time checked: 2021-12-12T15:12:18.188946\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T16:12:18.276012\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T17:12:18.339288\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T18:12:18.518603\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T19:12:18.652541\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T20:12:18.700651\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T21:12:18.921868\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T22:12:19.105100\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-12T23:12:19.235384\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-12\n",
      "\n",
      "Time checked: 2021-12-13T00:12:19.392194\n",
      "Date last run: 2021-12-12\n",
      "UTC date now is: 2021-12-13\n",
      "Item counts (university-wide):\n",
      "vu_total\n",
      "vu_men\n",
      "vu_women\n",
      "vu_orcid\n",
      "vu_works\n",
      "vu_men_works\n",
      "vu_women_works\n",
      "{'commit': Commit(sha=\"ad30af68730d192236ad7d29cf712958558c0f3c\"), 'content': ContentFile(path=\"vandycite/vandycite_item_data.csv\")}\n",
      "\n",
      "Item counts by unit:\n",
      "units_total\n",
      "{'commit': Commit(sha=\"87c9799c4cd39504f1e85e37a98da3143f8adf6e\"), 'content': ContentFile(path=\"vandycite/units_total.csv\")}\n",
      "units_women\n",
      "{'commit': Commit(sha=\"d8bfe449b99bfd3c7c6fe859eca806d414bf3f7d\"), 'content': ContentFile(path=\"vandycite/units_women.csv\")}\n",
      "units_men\n",
      "{'commit': Commit(sha=\"a61a759af6ae7d381225835f321d36fa5482a2bf\"), 'content': ContentFile(path=\"vandycite/units_men.csv\")}\n",
      "units_orcid\n",
      "{'commit': Commit(sha=\"1cafc6cc24c5d7695666be48410ce2b5e35a82eb\"), 'content': ContentFile(path=\"vandycite/units_orcid.csv\")}\n",
      "units_works\n",
      "{'commit': Commit(sha=\"e4135713905c22853d989055a35d5a89851486f8\"), 'content': ContentFile(path=\"vandycite/units_works.csv\")}\n",
      "units_works_men\n",
      "{'commit': Commit(sha=\"b9a21e3411f08ed98c784c9a1b9f74c5f697982c\"), 'content': ContentFile(path=\"vandycite/units_works_men.csv\")}\n",
      "units_works_women\n",
      "{'commit': Commit(sha=\"f568343a32f6dfec286255a26eab36b5af1f25ae\"), 'content': ContentFile(path=\"vandycite/units_works_women.csv\")}\n",
      "\n",
      "Contributions\n",
      "Clifford_Anderson\n",
      "Baskaufs\n",
      "Fmlester\n",
      "Ramonavromero\n",
      "Talinum\n",
      "Celiaswalker\n",
      "CatonMA2\n",
      "Gridersd\n",
      "JeffBTaylor\n",
      "Marjans74\n",
      "Charlotte_Y._Lew\n",
      "KukanaLuika\n",
      "Loyodea\n",
      "VanderBot\n",
      "{'commit': Commit(sha=\"39693592bbdbd959698b345ec236805768c3e6fa\"), 'content': ContentFile(path=\"vandycite/vandycite_edit_data.csv\")}\n",
      "Pages created\n",
      "Clifford_Anderson\n",
      "Baskaufs\n",
      "Fmlester\n",
      "Ramonavromero\n",
      "Talinum\n",
      "Celiaswalker\n",
      "CatonMA2\n",
      "Gridersd\n",
      "JeffBTaylor\n",
      "Marjans74\n",
      "Charlotte_Y._Lew\n",
      "KukanaLuika\n",
      "Loyodea\n",
      "VanderBot\n",
      "{'commit': Commit(sha=\"091d02fc68dd95116f04733e04a48f47ccfa2125\"), 'content': ContentFile(path=\"vandycite/vandycite_page_creation_data.csv\")}\n",
      "done\n",
      "\n",
      "Time checked: 2021-12-13T01:13:09.853787\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T02:13:10.035375\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T03:13:10.250494\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T04:13:10.368185\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T05:13:10.529257\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T06:13:10.656636\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T07:13:11.029840\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T08:13:11.184050\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T09:13:12.867002\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T10:13:13.004060\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T11:13:13.284474\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T12:13:13.456770\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T14:11:24.133346\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T15:11:25.327685\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T16:11:25.414603\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T17:11:26.402069\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T18:11:26.522328\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T20:15:32.684391\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T21:15:32.843124\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T22:20:29.528659\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-13T23:20:29.729753\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-13\n",
      "\n",
      "Time checked: 2021-12-14T00:20:30.174221\n",
      "Date last run: 2021-12-13\n",
      "UTC date now is: 2021-12-14\n",
      "Item counts (university-wide):\n",
      "vu_total\n",
      "vu_men\n",
      "vu_women\n",
      "vu_orcid\n",
      "vu_works\n",
      "vu_men_works\n",
      "vu_women_works\n",
      "{'commit': Commit(sha=\"a171ea4920d1bece438e916fe2e269547d4a82f3\"), 'content': ContentFile(path=\"vandycite/vandycite_item_data.csv\")}\n",
      "\n",
      "Item counts by unit:\n",
      "units_total\n",
      "{'commit': Commit(sha=\"507acd3768688bd5dce27a82bef40bb8238f77dd\"), 'content': ContentFile(path=\"vandycite/units_total.csv\")}\n",
      "units_women\n",
      "{'commit': Commit(sha=\"906eb9d752f2c01512a4280e05c99276071f6f68\"), 'content': ContentFile(path=\"vandycite/units_women.csv\")}\n",
      "units_men\n",
      "{'commit': Commit(sha=\"45157dcc9b30935f1f901f577f04fd477381ea47\"), 'content': ContentFile(path=\"vandycite/units_men.csv\")}\n",
      "units_orcid\n",
      "{'commit': Commit(sha=\"9f8d997cfee2caaabc56cc1a435aa11e016740b9\"), 'content': ContentFile(path=\"vandycite/units_orcid.csv\")}\n",
      "units_works\n",
      "{'commit': Commit(sha=\"f0d5e6b901581cb592ab6fb62e8f35cde27a9042\"), 'content': ContentFile(path=\"vandycite/units_works.csv\")}\n",
      "units_works_men\n",
      "{'commit': Commit(sha=\"83cc11e6f8b78468a074ff14a4c1c697d7b1223b\"), 'content': ContentFile(path=\"vandycite/units_works_men.csv\")}\n",
      "units_works_women\n",
      "{'commit': Commit(sha=\"1e25b3416639b4a630361bfdb50aea24b132e280\"), 'content': ContentFile(path=\"vandycite/units_works_women.csv\")}\n",
      "\n",
      "Contributions\n",
      "Clifford_Anderson\n",
      "Baskaufs\n",
      "Fmlester\n",
      "Ramonavromero\n",
      "Talinum\n",
      "Celiaswalker\n",
      "CatonMA2\n",
      "Gridersd\n",
      "JeffBTaylor\n",
      "Marjans74\n",
      "Charlotte_Y._Lew\n",
      "KukanaLuika\n",
      "Loyodea\n",
      "VanderBot\n",
      "{'commit': Commit(sha=\"2bbe232017e8e6c218fdc8b513d0f8dbf02f7590\"), 'content': ContentFile(path=\"vandycite/vandycite_edit_data.csv\")}\n",
      "Pages created\n",
      "Clifford_Anderson\n",
      "Baskaufs\n",
      "Fmlester\n",
      "Ramonavromero\n",
      "Talinum\n",
      "Celiaswalker\n",
      "CatonMA2\n",
      "Gridersd\n",
      "JeffBTaylor\n",
      "Marjans74\n",
      "Charlotte_Y._Lew\n",
      "KukanaLuika\n",
      "Loyodea\n",
      "VanderBot\n",
      "{'commit': Commit(sha=\"0fa52d6f1d917636096c0de5d09564ce5fc1eb44\"), 'content': ContentFile(path=\"vandycite/vandycite_page_creation_data.csv\")}\n",
      "done\n",
      "\n",
      "Time checked: 2021-12-14T01:21:16.838048\n",
      "Date last run: 2021-12-14\n",
      "UTC date now is: 2021-12-14\n",
      "\n",
      "Time checked: 2021-12-14T02:21:17.049632\n",
      "Date last run: 2021-12-14\n",
      "UTC date now is: 2021-12-14\n",
      "\n",
      "Time checked: 2021-12-14T03:21:17.210378\n",
      "Date last run: 2021-12-14\n",
      "UTC date now is: 2021-12-14\n",
      "\n",
      "Time checked: 2021-12-14T04:21:17.427314\n",
      "Date last run: 2021-12-14\n",
      "UTC date now is: 2021-12-14\n",
      "\n",
      "Time checked: 2021-12-14T05:21:17.670645\n",
      "Date last run: 2021-12-14\n",
      "UTC date now is: 2021-12-14\n",
      "\n",
      "Time checked: 2021-12-14T06:21:17.774021\n",
      "Date last run: 2021-12-14\n",
      "UTC date now is: 2021-12-14\n",
      "\n",
      "Time checked: 2021-12-14T07:21:17.954107\n",
      "Date last run: 2021-12-14\n",
      "UTC date now is: 2021-12-14\n",
      "\n",
      "Time checked: 2021-12-14T08:21:18.154626\n",
      "Date last run: 2021-12-14\n",
      "UTC date now is: 2021-12-14\n",
      "\n",
      "Time checked: 2021-12-14T09:21:18.289831\n",
      "Date last run: 2021-12-14\n",
      "UTC date now is: 2021-12-14\n",
      "\n",
      "Time checked: 2021-12-14T10:21:18.748485\n",
      "Date last run: 2021-12-14\n",
      "UTC date now is: 2021-12-14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True: # infinite loop\n",
    "    try:\n",
    "        print('Time checked:', datetime.datetime.utcnow().isoformat())\n",
    "\n",
    "        date_last_run = read_string_from_github_file(organization_name, repo_name, path_to_directory, 'last_run.txt')\n",
    "        print('Date last run:', date_last_run)\n",
    "\n",
    "        date_now_utc = generate_utc_date()\n",
    "        print('UTC date now is:', date_now_utc)\n",
    "\n",
    "        if date_now_utc > date_last_run:\n",
    "            # log into the GitHub API and create a repo instance\n",
    "            repo = login_get_repo(repo_name, github_username, organization_name, organization_is_user, cred_directory)\n",
    "\n",
    "            # Record today's Wikidata Vanderbilt-wide counts for items\n",
    "            print('Item counts (university-wide):')\n",
    "\n",
    "            # Retrieve old copy of data from GitHub\n",
    "            filename = 'vandycite_item_data.csv'\n",
    "            table = read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, filename)\n",
    "            # Query the Wikidata Query Service to get today's data\n",
    "            rawCsvText = get_vu_counts(table)\n",
    "            # Write edited data back to GitHub via its API\n",
    "            response = update_file(organization_name, repo_name, path_to_directory, filename, rawCsvText)\n",
    "            print(response)\n",
    "            print()\n",
    "\n",
    "            # Record today's Wikidata item counts by Vanderbilt subsidiary unit\n",
    "            print('Item counts by unit:')\n",
    "            queries = get_unit_affiliation_queries()\n",
    "\n",
    "            for query_dict in queries:\n",
    "                print(query_dict['name'])\n",
    "                filename = query_dict['name'] + '.csv'\n",
    "                table = read_lists_from_github_csv(organization_name, repo_name, path_to_directory, filename)\n",
    "                rawCsvText = add_query_to_unit_table(table, query_dict['query'])\n",
    "                #print(rawCsvText)\n",
    "                response = update_file(organization_name, repo_name, path_to_directory, filename, rawCsvText)\n",
    "                print(response)\n",
    "            print()\n",
    "\n",
    "            # Record today's counts of contrubutions to Wikidata by VandyCite members\n",
    "            print('Contributions')\n",
    "            filename = 'vandycite_edit_data.csv'\n",
    "            table = read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, filename)\n",
    "            rawCsvText = get_vandycite_contribution_counts(organization_name, repo_name, path_to_directory, table)\n",
    "            response = update_file(organization_name, repo_name, path_to_directory, filename, rawCsvText)\n",
    "            print(response)\n",
    "\n",
    "            # Record today's counts of pages created in Wikidata by VandyCite members\n",
    "            print('Pages created')\n",
    "            filename = 'vandycite_page_creation_data.csv'\n",
    "            table = read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, filename)\n",
    "            rawCsvText = get_vandycite_page_creation_counts(organization_name, repo_name, path_to_directory, table)\n",
    "            response = update_file(organization_name, repo_name, path_to_directory, filename, rawCsvText)\n",
    "            print(response)\n",
    "\n",
    "            # Update the date last run\n",
    "            response = update_file(organization_name, repo_name, path_to_directory, 'last_run.txt', generate_utc_date() )\n",
    "            print('done')\n",
    "        print()\n",
    "        # wait an hour before checking again\n",
    "        sleep(3600)\n",
    "    except:\n",
    "        print('Error occurred, trying again in 10 minutes')\n",
    "        sleep(600)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
