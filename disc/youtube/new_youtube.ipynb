{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication method take from https://www.thepythoncode.com/article/using-youtube-api-in-python\n",
    "\n",
    "# Notes carried over from previos version: you will have to use PIP to install the GitHub and \n",
    "# Google SDKs before these import statements will work.\n",
    "# The YouTube API is part of the family of Google APIs and uses Google's generic SDK\n",
    "# Use:\n",
    "# pip install google-api-python-client # Not sure if this is needed in this new version\n",
    "# pip install google-auth-oauthlib\n",
    "# pip install PyGithub\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import requests\n",
    "from time import sleep\n",
    "import csv\n",
    "import io\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# GitHub SDK\n",
    "from github import Github\n",
    "\n",
    "# Google API SDKs:\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "#import urllib.parse as p\n",
    "#import re\n",
    "\n",
    "# Control the location of credentials. \n",
    "# !!! It is unsafe to store credentials in the working directory if the code will be pushed to GitHub !!!\n",
    "cred_directory = 'home' # set to 'home' if the credential is in the home directory, otherwise working directory\n",
    "\n",
    "# Set configuration details necessary for interacting with the GitHub API\n",
    "\n",
    "# the access token should be generated for read/write access to public repos\n",
    "# see https://developer.github.com/v3/auth/#working-with-two-factor-authentication\n",
    "# see https://github.com/settings/tokens/new\n",
    "# select public_repo\n",
    "\n",
    "# reference on PyGithub: https://pygithub.readthedocs.io/en/latest/github_objects/Repository.html\n",
    "# reference on GitHub API: https://developer.github.com/v3/guides/getting-started/\n",
    "\n",
    "github_username = ''  # set to empty string if using a token (for 2FA)\n",
    "organization_name = 'heardlibrary'\n",
    "organization_is_user = False\n",
    "repo_name = 'dashboard'\n",
    "path_to_directory = 'disc/youtube/'\n",
    "\n",
    "# Set configuration details necessary for interacting with the YouTube Analytics API\n",
    "# You will need to modify the following line according to how you named your secrets file\n",
    "client_secrets_filename = 'codegraf_1-1_youtube_credentials.json'\n",
    "pickle_filename = 'youtube_token.pickle'\n",
    "\n",
    "if cred_directory == 'home':\n",
    "    home = str(Path.home()) #gets path to home directory; supposed to work for Win and Mac\n",
    "    client_secrets_file_path = home + '/' + client_secrets_filename\n",
    "    pickle_file_path = home + '/' + pickle_filename\n",
    "else:\n",
    "    cred_directory = 'working'\n",
    "    client_secrets_file_path = client_secrets_filename\n",
    "    pickle_file_path = pickle_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# utility functions\n",
    "# -----------------\n",
    "\n",
    "def generate_utc_date():\n",
    "    \"\"\"Generates today's date as an ISO 8601 formatted string.\"\"\"\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# RAW FILE FUNCTIONS\n",
    "\n",
    "def read_string_from_github_file(organization_name, repo_name, path_to_directory, filename):\n",
    "    \"\"\"Read raw string from a file in GitHub.\"\"\"\n",
    "    path = path_to_directory + filename\n",
    "    r = requests.get('https://raw.githubusercontent.com/' + organization_name + '/' + repo_name + '/master/' + path)\n",
    "    return r.text\n",
    "\n",
    "# LIST OF DICTIONARIES FUNCTIONS\n",
    "\n",
    "def read_dicts_from_csv(filename):\n",
    "    \"\"\"Read from a CSV file on disk into a list of dictionaries (representing a table).\"\"\"\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        table = []\n",
    "        for row in dict_object:\n",
    "            table.append(row)\n",
    "    return table\n",
    "\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    \"\"\"Write a list of dictionaries to a CSV file on disk.\"\"\"\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, filename):\n",
    "    \"\"\"Read from a CSV file in GitHub into a list of dictionaries (representing a table).\"\"\"\n",
    "    path = path_to_directory + filename\n",
    "    r = requests.get('https://raw.githubusercontent.com/' + organization_name + '/' + repo_name + '/master/' + path)\n",
    "    file_text = r.text.split('\\n')\n",
    "    file_rows = csv.DictReader(file_text)\n",
    "    table = []\n",
    "    for row in file_rows:\n",
    "        table.append(row)\n",
    "    return table\n",
    "\n",
    "def write_dicts_to_string(table, fieldnames):\n",
    "    \"\"\"Write a list of dictionaries to a CSV file using filestream.\"\"\"\n",
    "    output = io.StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in table:\n",
    "        writer.writerow(row)\n",
    "    return output.getvalue()\n",
    "\n",
    "# LIST OF LISTS FUNCTIONS\n",
    "\n",
    "def read_lists_from_github_csv(organization_name, repo_name, path_to_directory, filename):\n",
    "    \"\"\"Read from a CSV file in GitHub into a list of lists (representing a table).\"\"\"\n",
    "    path = path_to_directory + filename\n",
    "    r = requests.get('https://raw.githubusercontent.com/' + organization_name + '/' + repo_name + '/master/' + path)\n",
    "    file_text = r.text.split('\\n')\n",
    "    # remove any trailing newlines\n",
    "    if file_text[len(file_text)-1] == '':\n",
    "        file_text = file_text[0:len(file_text)-1]\n",
    "    file_rows = csv.reader(file_text)\n",
    "    table = []\n",
    "    for row in file_rows:\n",
    "        table.append(row)\n",
    "    return table\n",
    "\n",
    "def write_lists_to_csv(file_name, array):\n",
    "    \"\"\"Write a list of lists to a CSV file on disk.\"\"\"\n",
    "    with open(file_name, 'w', newline='', encoding='utf-8') as file_object:\n",
    "        writer_object = csv.writer(file_object)\n",
    "        for row in array:\n",
    "            writer_object.writerow(row)\n",
    "\n",
    "def write_lists_to_string(table):\n",
    "    \"\"\"Write a list of lists to a CSV file using filestream.\"\"\"\n",
    "    output = io.StringIO()\n",
    "    writer = csv.writer(output)\n",
    "    for row in table:\n",
    "        writer.writerow(row)\n",
    "    return output.getvalue()\n",
    "\n",
    "# -----------------\n",
    "# functions for interacting with GitHub\n",
    "# -----------------\n",
    "\n",
    "# value of directory should be either 'home' or 'working'\n",
    "def load_credential(filename, directory):\n",
    "    cred = ''\n",
    "    # to change the script to look for the credential in the working directory, change the value of home to empty string\n",
    "    if directory == 'home':\n",
    "        home = str(Path.home()) #gets path to home directory; supposed to work for Win and Mac\n",
    "        credential_path = home + '/' + filename\n",
    "    else:\n",
    "        directory = 'working'\n",
    "        credential_path = filename\n",
    "    try:\n",
    "        with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "            cred = file_object.read()\n",
    "    except:\n",
    "        print(filename + ' file not found - is it in your ' + directory + ' directory?')\n",
    "        exit()\n",
    "    return(cred)\n",
    "\n",
    "# pass in an empty string for organization_name to use an individual account\n",
    "# pass in an empty string for github_username to use a token instead of username login\n",
    "def login_get_repo(repo_name, github_username, organization_name, organization_is_user, cred_directory):\n",
    "    if github_username == '':\n",
    "        token = load_credential('linked-data_github_token.txt', cred_directory)\n",
    "        g = Github(login_or_token = token)\n",
    "    else:\n",
    "        pwd = load_credential('pwd.txt', cred_directory)\n",
    "        g = Github(github_username, pwd)\n",
    "    \n",
    "    if organization_is_user:\n",
    "        # this option accesses a user's repo instead of an organizational one\n",
    "        # In this case, the value of organization_name is not used.\n",
    "        user = g.get_user()\n",
    "        repo = user.get_repo(repo_name)\n",
    "    else:\n",
    "        # this option creates an instance of a repo in an organization\n",
    "        # to which the token creator has push access\n",
    "        organization = g.get_organization(organization_name)\n",
    "        repo = organization.get_repo(repo_name)\n",
    "    return(repo)\n",
    "\n",
    "def get_user_list(repo):\n",
    "    person_list = []\n",
    "    people = repo.get_collaborators()\n",
    "    for person in people:\n",
    "        person_list.append(person.login)\n",
    "    return person_list\n",
    "\n",
    "def get_file_sha(account, repo, file_path):\n",
    "    # get the data about the file to get its blob SHA\n",
    "\n",
    "    r = requests.get('https://api.github.com/repos/' + account + '/' + repo + '/contents/' + file_path)\n",
    "    file_data = r.json()\n",
    "    try:\n",
    "        sha = file_data['sha']\n",
    "    except:\n",
    "        # if the file doesn't already exist on GitHub, no sha will be returned\n",
    "        sha = ''\n",
    "    return sha\n",
    "\n",
    "# use this function to update an existing text file\n",
    "def update_file(repo, account, repo_name, path_to_directory, filename, content):\n",
    "    path = path_to_directory + filename\n",
    "    commit_message = 'Update ' + filename + ' file via API'\n",
    "    sha = get_file_sha(account, repo_name, path)\n",
    "    if sha == '':\n",
    "        response = repo.create_file(path, commit_message, content)\n",
    "    else:\n",
    "        response = repo.update_file(path, commit_message, content, sha)\n",
    "    return response\n",
    "\n",
    "# -----------------\n",
    "# functions for interacting with the YouTube API\n",
    "# -----------------\n",
    "\n",
    "def youtube_authenticate():\n",
    "    \"\"\"Performs authentication with Google's API.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Stores the access tokens locally in pickled form.\n",
    "    If the pickle token doesn't exist or is expired, performs an authorization with user via webpage.\n",
    "    \"\"\"\n",
    "    # Disable OAuthlib's HTTPs verification when running locally.\n",
    "    # *DO NOT* leave this option enabled when running in production.\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = '1'\n",
    "    api_service_name = 'youtubeAnalytics'\n",
    "    api_version = 'v2'\n",
    "    scopes = ['https://www.googleapis.com/auth/youtube.readonly']\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first time\n",
    "    if os.path.exists(pickle_file_path):\n",
    "        with open(pickle_file_path, 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # if there are no (valid) credentials availablle, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(client_secrets_file_path, scopes)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # save the credentials for the next run\n",
    "        with open(pickle_file_path, 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    return build(api_service_name, api_version, credentials=creds)\n",
    "\n",
    "# -----------------\n",
    "# High level functions\n",
    "# -----------------\n",
    "\n",
    "def built_video_filter_string(video_metadata_filename):\n",
    "    \"\"\"Load video data and build the filter string from video IDs.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Under the v2 API, the video filter can send up to 500 IDs. \n",
    "    As of 2021-02-02, I have 268 videos I'm tracking, so at some point \n",
    "    this may have to be broken into two or more API calls.\n",
    "    \"\"\"\n",
    "    metadata = read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, video_metadata_filename)\n",
    "    filter_string = 'video=='\n",
    "    output_header_list = ['date']\n",
    "    count = 0\n",
    "    for video in metadata:\n",
    "    #for video in metadata[0:5]: # switch to this line for testing\n",
    "        count += 1\n",
    "        if count > 500:\n",
    "            print('Warning: limit of 500 videos exceeded! Modify the script.')\n",
    "            break\n",
    "        filter_string += video['id'].strip() + ','\n",
    "        output_header_list.append(video['id'].strip())\n",
    "    # remove final trailing comma\n",
    "    filter_string = filter_string[:len(filter_string)-1]\n",
    "    return filter_string, output_header_list\n",
    "\n",
    "def get_youtube_usage_stats(todays_date_utc, filter_string):\n",
    "    \"\"\"Perform the call to the YouTube Analytics API.\"\"\"\n",
    "    print('sending request to YouTube Analytics API')\n",
    "    \n",
    "    result = youtube.reports().query(\n",
    "        ids='channel==MINE',\n",
    "        startDate='2013-01-01', # don't have any videos dated earlier than that\n",
    "        endDate=todays_date_utc,\n",
    "        metrics='estimatedMinutesWatched,views',\n",
    "        filters=filter_string,\n",
    "        dimensions='video'\n",
    "    ).execute()\n",
    "        \n",
    "    #print(json.dumps(result, indent=2))\n",
    "    print('done retrieving data from YouTube API')\n",
    "    return result\n",
    "\n",
    "# The revised tables are then pushed to GitHub\n",
    "# The first column must be the date.\n",
    "def add_data_to_tables():\n",
    "    \"\"\"Retrieve data for counts and minutes, then append row to table, push revised table to GitHub\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The first column must be the date.\n",
    "    \"\"\"\n",
    "    todays_date_utc = generate_utc_date()\n",
    "    filter_string, output_header_list = built_video_filter_string('video-metadata.csv')\n",
    "    \n",
    "    minutes_table = read_lists_from_github_csv(organization_name, repo_name, path_to_directory, 'total_minutes_watched.csv')\n",
    "    views_table = read_lists_from_github_csv(organization_name, repo_name, path_to_directory, 'total_views.csv')\n",
    "\n",
    "    # Check to make sure that there are the same number of videos in the metadata list and the tables\n",
    "    # If not, nothing happens\n",
    "    if len(minutes_table[0]) != len(output_header_list):\n",
    "        print('minutes table:', len(minutes_table[0]), ' header list:', len(output_header_list))\n",
    "        print('Warning! Minutes table does not have the same number of videos as the videos metadata table!')\n",
    "        return\n",
    "    \n",
    "    if len(views_table[0]) != len(output_header_list):\n",
    "        print('views table:', len(views_table[0]), ' header list:', len(output_header_list))\n",
    "        print('Warning! Views table does not have the same number of videos as the videos metadata table!')\n",
    "        return\n",
    "    \n",
    "    tries = 0\n",
    "    success = False\n",
    "\n",
    "    # try to acquire the data for an hour\n",
    "    while (success == False) and (tries < 12):\n",
    "        try:\n",
    "            results = get_youtube_usage_stats(todays_date_utc, filter_string)\n",
    "            api_data = results['rows']\n",
    "            #print(api_data)\n",
    "\n",
    "            #dictionary = get_unit_counts(query)\n",
    "            success = True\n",
    "            \n",
    "            # We start the new row with the date (first column)\n",
    "            minutes_row = [todays_date_utc]\n",
    "            views_row = [todays_date_utc]\n",
    "\n",
    "            # The video IDs from the API data are compared with the column headers from the minutes table.\n",
    "            # We are assuming that all videos in the video metadata table are found in the column headers.\n",
    "            for header in output_header_list[1:]: # skip the first item (date)\n",
    "                found = False\n",
    "                \n",
    "                # Step through the API records and match with the header\n",
    "                for video in api_data:\n",
    "                    if video[0] == header:\n",
    "                        found = True\n",
    "                        minutes_row.append(str(video[1]))\n",
    "                        views_row.append(str(video[2]))\n",
    "                        break\n",
    "                # In the case where the videos metadata table has a video not in the API results, it's added as a blank cell\n",
    "                if not found:\n",
    "                    minutes_row.append('0')\n",
    "                    views_row.append('0')\n",
    "        except:\n",
    "            tries += 1\n",
    "            sleep(300) # wait 5 minutes and try again\n",
    "\n",
    "    if success:\n",
    "        # log into the GitHub API and create a repo instance\n",
    "        repo = login_get_repo(repo_name, github_username, organization_name, organization_is_user, cred_directory)\n",
    "        \n",
    "        minutes_table.append(minutes_row)\n",
    "        rawCsvText = write_lists_to_string(minutes_table)\n",
    "        response = update_file(repo, organization_name, repo_name, path_to_directory, 'total_minutes_watched.csv', rawCsvText)\n",
    "        print('minutes response: ')\n",
    "        print(response)\n",
    "\n",
    "        views_table.append(views_row)\n",
    "        rawCsvText = write_lists_to_string(views_table)\n",
    "        response = update_file(repo, organization_name, repo_name, path_to_directory, 'total_views.csv', rawCsvText)\n",
    "        print('views response: ')\n",
    "        print(response)\n",
    "\n",
    "        # Update the date last run\n",
    "        response = update_file(repo, organization_name, repo_name, path_to_directory, 'last_run.txt', generate_utc_date() )\n",
    "        print('done')\n",
    "\n",
    "        #write_lists_to_csv('total_minutes_watched_test.csv', minutes_table)\n",
    "        #write_lists_to_csv('total_views_test.csv', views_table)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Authentication done:', datetime.datetime.utcnow().isoformat())\n",
    "# authenticate to YouTube API\n",
    "youtube = youtube_authenticate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True: # infinite loop\n",
    "    try:\n",
    "        print('Time checked:', datetime.datetime.utcnow().isoformat())\n",
    "\n",
    "        date_last_run = read_string_from_github_file(organization_name, repo_name, path_to_directory, 'last_run.txt')\n",
    "        print('Date last run:', date_last_run)\n",
    "\n",
    "        date_now_utc = generate_utc_date()\n",
    "        print('UTC date now is:', date_now_utc)\n",
    "\n",
    "        #if date_now_utc > date_last_run:\n",
    "        if True:\n",
    "            add_data_to_tables()\n",
    "        print()\n",
    "        # wait an hour before checking again\n",
    "        sleep(3600)\n",
    "    except Exception as ex:\n",
    "        print('Error occurred, trying again in 10 minutes')\n",
    "        print(type(ex).__name__, ex.args)\n",
    "        sleep(600)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
